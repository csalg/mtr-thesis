{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdbe86e",
   "metadata": {},
   "source": [
    "### Continuous assessment model\n",
    "\n",
    "In the methodology section, it was theorized that if we fix the number of exposures, the resulting memory strength will vary depending on each user's ability at a specific time. This is a reasonable assumption, backed by psychological research. Following this line of thought, it follows that the data is not smooth, because given $x$ and $x'$ such that they are near (or, indeed, identical) two different users may produce wildly different $y$ and $y'$.\n",
    "\n",
    "In addition to theoretical results, several experiments also support this line of reasoning (these experiments are implemented in two other notebooks):\n",
    "1. Using only the earliest 10% of the data to predict the remaining 90% shows how the performance degrades as time passes (table & plot).\n",
    "2. One-user-out experiment: how well can a model trained using all users generalize to unseen users? (table)\n",
    "\n",
    "The continuous assessment model is a meta-learning heuristic that addresses these serious limitations:\n",
    "1. All the data is clustered using GMMs.\n",
    "2. The means are used as the _assessment input_, or $X_{A}$.\n",
    "3. Whenever a new segment of data is input into the system, first train a MTR model on the new data and use that model to predict the memory strengths of $X_{A}$. Take the weighted average of these averages, using the proportion of points in each cluster as the weight. Denote this weighted average as the _assessment score_.\n",
    "4. Cluster the assessment scores into $m$ clusters, where $m$ is the desired number of models. In the experiment below $m=3$, but this parameter can be increased given more data.\n",
    "5. Now we have a mapping from the datasets to groups of clusters, each of which can be used to fit their own MTR model. \n",
    "\n",
    "The assumption is that this will lead to grouping user/time datasets of similar ability together, but this is only a heuristic and there is no guarantee that the solution will be optimal. Optimal solutions can be derived by brute force, either by backtracking or branch-and-bound, but this is not scalable. Essentially, this is a blind source separation problem.\n",
    "\n",
    "The rest of the notebook implements the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b96ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
